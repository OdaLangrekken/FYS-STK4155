{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decb8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_path = os.getcwd()\n",
    "sys.path.append(current_path + '\\..')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import self-made modules\n",
    "from project2_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b7a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from project2_code import gradient_descent, stochastic_gradient_descent, cost_linear, cross_entropy, sigmoid, sigmoid_derivative\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    \"\"\"\n",
    "    A class to build a Neural Network. The network is trained using forward and backward propagation.\n",
    "    The network can be used both for classification and for regression\n",
    "\n",
    "    Attributes\n",
    "    ----\n",
    "       data (dataframe or numpy array): input data\n",
    "       target (array): target\n",
    "       regr_or_class (string): 'regr' if regression (default) and 'class' if classification\n",
    "       activation (string): activation function to use, default is 'sigmoid'\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data, target, regr_or_class='regr', activation = 'sigmoid'):\n",
    "        self.target = target\n",
    "        self.num_inputs = data.shape[0]\n",
    "        self.num_features = data.shape[1]\n",
    "        self.weights = {}  # Dictionary to store weights for each layer\n",
    "        self.biases = {} # Dictionary to store biases\n",
    "        self.activations = {} # Dictionary to store input for each layer\n",
    "        self.activations[0] = data.to_numpy() \n",
    "        self.errors = {} # Store errors\n",
    "        self.error_matrix = {} # Store errors for all layers\n",
    "        # Use cross-entropy as cost function if classification, and mean squared error is regression\n",
    "        if regr_or_class == 'regr':\n",
    "            self.cost_function = cost_linear\n",
    "            self.last_activation = None\n",
    "            self.learning_type = 'regr'\n",
    "        elif regr_or_class == 'class':\n",
    "            self.cost_function = cross_entropy\n",
    "            self.learning_type = 'class'\n",
    "        # Check what activation function to use\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation_function = sigmoid\n",
    "        \n",
    "    def initialize_weights(self, size_layer, size_prev_layer, n):\n",
    "        # Use the He initializing for weights. Weights drawn from guassian dist with std sqrt(2/n)\n",
    "        return np.random.randn(size_prev_layer, size_layer) * np.sqrt(2/n)\n",
    "    \n",
    "    def initialize_bias(self, size_layer):\n",
    "        return np.zeros(size_layer) + 0.01\n",
    "        \n",
    "    def add_layer(self, size_layer):\n",
    "        \"\"\"\n",
    "        Adds layer of size size_layer (bias excluded)\n",
    "        \"\"\"\n",
    "        # Check if this is first layer\n",
    "        if len(self.weights) == 0:\n",
    "            self.weights[0] = self.initialize_weights(size_layer, self.num_features, self.num_inputs)\n",
    "            self.biases[0] = self.initialize_bias(size_layer)\n",
    "            self.error_matrix[0] = np.zeros((size_layer, self.num_features))\n",
    "        else:\n",
    "            counter = len(self.weights)\n",
    "            size_prev_layer = self.weights[counter - 1].shape[1]\n",
    "            self.weights[counter] = self.initialize_weights(size_layer, size_prev_layer, self.num_inputs)\n",
    "            self.biases[counter] = self.initialize_bias(size_layer)\n",
    "            \n",
    "    def compute_z(self, current_layer):\n",
    "            #Get weights and bias for current layer\n",
    "            weights = self.weights.get(current_layer)\n",
    "            bias = self.biases.get(current_layer)\n",
    "            #Get input data for current layer\n",
    "            inputs = self.activations.get(current_layer)\n",
    "            #Calculate weighted sum of input and add bias\n",
    "            z = inputs @ weights + bias\n",
    "            return z\n",
    "            \n",
    "    def forward_prop(self):\n",
    "        \"\"\"\n",
    "        Propagates the input through all layers.\n",
    "        For each layer the function gets the layer weights and input data from the weights and activations dictionaries.\n",
    "        The code calculates the matrix product weights @ inputs and adds the bias.\n",
    "        An activation function is used to get the output, and the output is stored to be used as input for the next layer,\n",
    "        or as the final output for the final layer.\n",
    "        \"\"\"\n",
    "        current_layer = 0\n",
    "        num_layers = len(self.weights)\n",
    "        # Loop through all layers\n",
    "        while current_layer < num_layers:\n",
    "            # Find z\n",
    "            z = self.compute_z(current_layer)\n",
    "            #Calculate output using activation function\n",
    "            # Check if last layer\n",
    "            if current_layer == num_layers - 1:\n",
    "                if self.last_activation:\n",
    "                    a = self.last_activation(z)\n",
    "                else:\n",
    "                    a = z\n",
    "            else:\n",
    "                a = self.activation_function(z)\n",
    "            #Store output to use as input for next layer\n",
    "            current_layer += 1\n",
    "            self.activations[current_layer] = a\n",
    "\n",
    "    def back_prop(self, y):\n",
    "        # Start with last layer\n",
    "        current_layer = len(self.weights)\n",
    "        # Get output for last layer\n",
    "        a = self.activations.get(current_layer)\n",
    "        # Derivative of cost function for MSE\n",
    "        C_deriv = (a - y)\n",
    "        # Get derivative of activation function\n",
    "        if self.learning_type == 'regr':\n",
    "            activation_deriv = 1\n",
    "        elif self.learning_type == 'class':\n",
    "            z = self.compute_z(current_layer)\n",
    "            activation_deriv = sigmoid_derivate(z)\n",
    "        output_error = C_deriv * activation_deriv\n",
    "        # Store error for last layer\n",
    "        self.errors[current_layer] = output_error\n",
    "        current_layer -= 1\n",
    "        while current_layer > 0:\n",
    "            error_prev = self.errors[current_layer + 1]\n",
    "            weights = self.weights[current_layer]\n",
    "            if self.learning_type == 'regr':\n",
    "                activation_deriv = 1\n",
    "            elif self.learning_type == 'class':\n",
    "                z = self.compute_z(current_layer)\n",
    "                activation_deriv = sigmoid_derivate(z)\n",
    "            \n",
    "    def update_weights(self, learning_rate, reg_coef):\n",
    "        current_layer = 0\n",
    "        while current_layer < len(self.weights):\n",
    "            m = self.num_inputs\n",
    "            size = self.weights[current_layer].shape\n",
    "            gradient = np.zeros(size)\n",
    "            gradient[:, 0] = 1/m * self.error_matrix[current_layer][:, 0]\n",
    "            gradient[:, 1:] = 1/m * (self.error_matrix[current_layer][:, 1:] + reg_coef*self.weights[current_layer][:, 1:])\n",
    "            self.weights[current_layer] -= learning_rate * gradient\n",
    "            current_layer += 1\n",
    "        \n",
    "    def train(self, num_epochs = 100, learning_rate = 1, reg_coef = 0):\n",
    "        for i in range(num_epochs):\n",
    "            # Get output by using feed forward\n",
    "            self.forward_prop()\n",
    "            # Propagate error using back propagation\n",
    "            self.back_prop(self.target)\n",
    "            # Update the weights\n",
    "            #self.update_weights(learning_rate, reg_coef)\n",
    "            #if i % 100 == 0:\n",
    "            #    print(f'Epochs done: {i}/{num_epochs}')\n",
    "            \n",
    "    def predict(self, x):\n",
    "        # Set input data\n",
    "        self.activations[0] = x\n",
    "        preds = self.activations[len(self.activations) - 1]\n",
    "        if self.learning_type == 'class':\n",
    "            return np.argmax(preds, axis = 0)\n",
    "        return preds.to_numpy()\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        self.forward_prop(x)\n",
    "        preds = self.activations[len(self.activations) - 1]\n",
    "        return preds\n",
    "    \n",
    "    def accuracy(self, y_pred, y):\n",
    "        acc = 0\n",
    "        for i in range(len(y)):\n",
    "            if y_pred[i] == y[i]:\n",
    "                acc += 1\n",
    "        return acc/len(y)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = np.random.rand(n)\n",
    "\n",
    "# Create polynomial function of x, up to a degree of 5\n",
    "y = simple_polynomial(x, polynomial_degree = 2)\n",
    "X = create_design_matrix_1d(x, 2)\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4c4e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,4) and (700,700) not aligned: 4 (dim 1) != 700 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-31583b1712f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ddc3894b0a8a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_epochs, learning_rate, reg_coef)\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;31m# Propagate error using back propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[1;31m# Update the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;31m#self.update_weights(learning_rate, reg_coef)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ddc3894b0a8a>\u001b[0m in \u001b[0;36mback_prop\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0merror_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[1;31m# Remove error corresponding to bias unit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,4) and (700,700) not aligned: 4 (dim 1) != 700 (dim 0)"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork(X_train, y_train)        \n",
    "network.add_layer(4)\n",
    "network.add_layer(1)\n",
    "network.train()\n",
    "y_pred = network.predict(X_train)\n",
    "MSE(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf989be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# display images in notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(\"inputs = (n_inputs, pixel_width, pixel_height) = \" + str(inputs.shape))\n",
    "print(\"labels = (n_inputs) = \" + str(labels.shape))\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(\"X = (n_inputs, n_features) = \" + str(inputs.shape))\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[random_indices[i]])\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# one-liner from scikit-learn library\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)\n",
    "\n",
    "\n",
    "print(\"Number of training images: \" + str(len(X_train)))\n",
    "print(\"Number of test images: \" + str(len(X_test)))\n",
    "\n",
    "# building our neural network\n",
    "\n",
    "n_inputs, n_features = X_train.shape\n",
    "n_hidden_neurons = 50\n",
    "n_categories = 10\n",
    "\n",
    "# we make the weights normally distributed using numpy.random.randn\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "print(hidden_weights.shape)\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452fefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the feed-forward pass, subscript h = hidden layer\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def feed_forward(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z_h = np.matmul(X, hidden_weights) + hidden_bias\n",
    "    print(np.matmul(X, hidden_weights))\n",
    "    print(z_h)\n",
    "    print('hei')\n",
    "    print(z_h.shape)\n",
    "    # activation in the hidden layer\n",
    "    a_h = sigmoid(z_h)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z_o)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "probabilities = feed_forward(X_train)\n",
    "print(\"probabilities = (n_inputs, n_categories) = \" + str(probabilities.shape))\n",
    "print(\"probability that image 0 is in category 0,1,2,...,9 = \\n\" + str(probabilities[0]))\n",
    "print(\"probabilities sum up to: \" + str(probabilities[0].sum()))\n",
    "print()\n",
    "\n",
    "# we obtain a prediction by taking the class with the highest likelihood\n",
    "def predict(X):\n",
    "    probabilities = feed_forward(X)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "predictions = predict(X_train)\n",
    "print(\"predictions = (n_inputs) = \" + str(predictions.shape))\n",
    "print(\"prediction for image 0: \" + str(predictions[0]))\n",
    "print(\"correct label for image 0: \" + str(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88408f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
