{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decb8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_path = os.getcwd()\n",
    "sys.path.append(current_path + '\\..')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Import self-made modules\n",
    "from project2_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ccd306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from project2_code import gradient_descent, stochastic_gradient_descent, cost_linear, cross_entropy\n",
    "from project2_code import sigmoid, sigmoid_derivative, relu, relu_derivative, relu_leaky, relu_leaky_derivative\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    \"\"\"\n",
    "    A class to build a Neural Network. The network is trained using forward and backward propagation.\n",
    "    The network can be used both for classification and for regression\n",
    "\n",
    "    Attributes\n",
    "    ----\n",
    "       num_features (int): number of features in input data. Needed to build first layer\n",
    "       regr_or_class (string): 'regr' if regression (default) and 'class' if classification\n",
    "       activation (string): activation function to use, default is 'sigmoid'\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, regr_or_class='regr', activation = 'sigmoid'):\n",
    "        self.num_features = num_features\n",
    "        self.weights = {}  # Dictionary to store weights for each layer\n",
    "        self.biases = {} # Dictionary to store biases\n",
    "        self.activations = {} # Dictionary to store input for each layer\n",
    "        self.errors = {} # Store errors\n",
    "        # Check if we're doing regression or classification\n",
    "        self.learning_type = regr_or_class\n",
    "        # Check what activation function to use\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation_function = sigmoid\n",
    "            self.activation_prime = sigmoid_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation_function = relu\n",
    "            self.activation_prime = relu_derivative\n",
    "        elif activation == 'leakyrelu':\n",
    "            self.activation_function = relu_leaky\n",
    "            self.activation_prime = relu_leaky_derivative\n",
    "        \n",
    "    def initialize_weights(self, size_layer, size_prev_layer):\n",
    "        \"\"\"\n",
    "        Initializes weights by drawing from a gaussian distribution.\n",
    "        \n",
    "        Input\n",
    "        ------\n",
    "        size_prev_layer (int): number of nodes in previous layer\n",
    "        size_layer (int): number of nodes in this layer\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        2d array of size size_prev_layer x size_layer\n",
    "        \"\"\"\n",
    "        return np.random.randn(size_prev_layer, size_layer) \n",
    "    \n",
    "    def initialize_bias(self, size_layer):\n",
    "        \"\"\"\n",
    "        Initializes biases for all nodes by drawing a random number between 0 and 1\n",
    "        \n",
    "        Input\n",
    "        ------\n",
    "        size_layer (int): number of nodes in this layer\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        1d array of size size_layer\n",
    "        \"\"\"\n",
    "        return np.random.rand(size_layer)\n",
    "        \n",
    "    def add_layer(self, size_layer):\n",
    "        \"\"\"\n",
    "        Adds layer of size 'size_layer' to the network. Initializes weights and biases for all layers.\n",
    "        \n",
    "        Input\n",
    "        ------\n",
    "        size_layer (int): number of nodes in this layer\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Check if this is first layer\n",
    "        if len(self.weights) == 0:\n",
    "            self.weights[0] = self.initialize_weights(size_layer, self.num_features)\n",
    "            self.biases[0] = self.initialize_bias(size_layer)\n",
    "        else:\n",
    "            counter = len(self.weights)\n",
    "            size_prev_layer = self.weights[counter - 1].shape[1]\n",
    "            self.weights[counter] = self.initialize_weights(size_layer, size_prev_layer)\n",
    "            self.biases[counter] = self.initialize_bias(size_layer)\n",
    "            \n",
    "    def compute_z(self, current_layer):\n",
    "        \"\"\"\n",
    "        Computes the weighted sum inputs @ weights and add bias.\n",
    "        \"\"\"\n",
    "        #Get weights and bias for current layer\n",
    "        weights = self.weights.get(current_layer)\n",
    "        bias = self.biases.get(current_layer)\n",
    "        #Get input data for current layer\n",
    "        inputs = self.activations.get(current_layer)\n",
    "        #Calculate weighted sum of input and add bias\n",
    "        z = inputs @ weights + bias\n",
    "        return z\n",
    "            \n",
    "    def forward_prop(self):\n",
    "        \"\"\"\n",
    "        Propagates the input through all layers.\n",
    "        For each layer the function gets the layer weights and input data from the weights and activations dictionaries.\n",
    "        The code calculates the matrix product weights @ inputs and adds the bias.\n",
    "        An activation function is used to get the output, and the output is stored to be used as input for the next layer,\n",
    "        or as the final output for the final layer.\n",
    "        \"\"\"\n",
    "        current_layer = 0\n",
    "        num_layers = len(self.weights)\n",
    "        # Loop through all layers\n",
    "        while current_layer < num_layers:\n",
    "            # Find z\n",
    "            z = self.compute_z(current_layer)\n",
    "            # Check if we are on last layer\n",
    "            # If last layer check if we are doing classification or regression\n",
    "            # If regression use a linear activation function for last layer\n",
    "            # If classification use the sigmoid function\n",
    "            # Use the activation function for other layers\n",
    "            if current_layer == num_layers - 1:\n",
    "                if self.learning_type == 'class':\n",
    "                    a = sigmoid(z)\n",
    "                else:\n",
    "                    a = z\n",
    "            else:\n",
    "                a = self.activation_function(z)\n",
    "            #Store output to use as input for next layer\n",
    "            current_layer += 1\n",
    "            self.activations[current_layer] = a\n",
    "\n",
    "    def back_prop(self, y):\n",
    "        \"\"\"\n",
    "        Propagates the error backwards through all layers. Finds first error for last layer using error from cost function.\n",
    "        Error is then propagated backwards untill first layer is reached. The function updates the error dictionary.\n",
    "        \"\"\"\n",
    "        # Start with last layer\n",
    "        current_layer = len(self.weights)\n",
    "        # Get output for last layer\n",
    "        a = self.activations.get(current_layer).ravel()\n",
    "        # Derivative of cost function for MSE\n",
    "        C_deriv = (a - y).reshape(-1, 1)\n",
    "        # Get derivative of activation function used in last layer\n",
    "        z = self.compute_z(current_layer-1)\n",
    "        if self.learning_type == 'regr':\n",
    "            activation_deriv = np.ones((len(z), 1))\n",
    "        elif self.learning_type == 'class':\n",
    "            activation_deriv = sigmoid_prime(z)\n",
    "        output_error = C_deriv * activation_deriv\n",
    "        # Store error for last layer\n",
    "        self.errors[current_layer] = output_error\n",
    "        current_layer -= 1\n",
    "        # Loop through all layers\n",
    "        while current_layer > 0:\n",
    "            error_prev = self.errors[current_layer + 1]\n",
    "            weights = self.weights[current_layer]\n",
    "            z = self.compute_z(current_layer-1)\n",
    "            activation_deriv = self.activation_prime(z)\n",
    "            # Find error for current layer\n",
    "            error = np.dot(error_prev, weights.T) * activation_deriv\n",
    "            # Store error\n",
    "            self.errors[current_layer] = error\n",
    "            current_layer -= 1\n",
    "            \n",
    "    def update_weights(self, learning_rate, reg_coef):\n",
    "        \"\"\"\n",
    "        Updates weight based on error from backpropagation.\n",
    "        \"\"\"\n",
    "        current_layer = 0\n",
    "        while current_layer < len(self.weights):\n",
    "            activations = self.activations[current_layer]\n",
    "            error = self.errors[current_layer + 1]\n",
    "            self.weights[current_layer] = self.weights[current_layer] - learning_rate*np.dot(activations.T, error)\n",
    "            self.biases[current_layer] = self.biases[current_layer] - learning_rate*np.sum(error, axis=0)\n",
    "            current_layer += 1\n",
    "        \n",
    "    def train(self, data, target, num_epochs = 100, minibatches = 1, learning_rate = 0.01, reg_coef = 0): \n",
    "        \"\"\"\n",
    "        This function trains the network. For every epoch the input is propagated through the network to get the\n",
    "        final output, and the error is propagated backwards. The weights are updated. This is repeated for num_epochs iterations.\n",
    "        \n",
    "        Input \n",
    "        -------\n",
    "        data (dataframe / array): input data\n",
    "        target (array): target\n",
    "        num_epochs (int): number of epochs to train the network with forward and backward propagation\n",
    "        minibatches (int): number of batches\n",
    "        learning_rate (float): learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        # Find size of each minibatch\n",
    "        n = len(data)  # number of rows\n",
    "        batch_size = int(n/minibatches)\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            # Shuffle data\n",
    "            data_shuffle, target_shuffle = shuffle(data, target)\n",
    "            # Choose random batch\n",
    "            batch_chosen = np.random.randint(0, minibatches)\n",
    "            data_minibatch = data_shuffle[batch_chosen:batch_chosen+batch_size]\n",
    "            target_minibatch = target_shuffle[batch_chosen:batch_chosen+batch_size]\n",
    "            # Set minibatch data as input to first layer\n",
    "            self.activations[0] = data_minibatch\n",
    "            # Get output by using feed forward\n",
    "            self.forward_prop()\n",
    "            # Propagate error using back propagation\n",
    "            self.back_prop(target_minibatch)\n",
    "            # Update the weights\n",
    "            self.update_weights(learning_rate, reg_coef)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the output of new data.\n",
    "        Works for both classification (output is categorical) and regression (output is continuous)\n",
    "        \"\"\"\n",
    "        # Set input data\n",
    "        self.activations[0] = x\n",
    "        # Propagate input\n",
    "        self.forward_prop()\n",
    "        preds = self.activations[len(self.activations) - 1].ravel()\n",
    "        if self.learning_type == 'class':\n",
    "            return np.argmax(preds, axis = 0)\n",
    "        return preds\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        Predicts probability of each class. Used for classification\n",
    "        \"\"\"\n",
    "        self.forward_prop(x)\n",
    "        preds = self.activations[len(self.activations) - 1]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = 10*np.random.rand(n)\n",
    "# Create polynomial function of x, up to a degree of 5\n",
    "y = simple_polynomial(x, polynomial_degree = 2)\n",
    "y = 2 + 5*x + x**2\n",
    "X = create_design_matrix_1d(x, 2)\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4c4e48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.941323893936476"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = NeuralNetwork(X_train_scaled.shape[1])        \n",
    "network.add_layer(4)\n",
    "network.add_layer(4)\n",
    "network.add_layer(1)\n",
    "network.train(X_train_scaled, y_train, minibatches=2, num_epochs = 1000, learning_rate=0.0001)\n",
    "y_pred = network.predict(X_train_scaled)\n",
    "MSE(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6764199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.34596315798952"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = network.predict(X_test_scaled)\n",
    "MSE(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e98dc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1925.6065069616507"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = NeuralNetwork(X_train_scaled.shape[1], activation='relu')        \n",
    "network.add_layer(4)\n",
    "network.add_layer(4)\n",
    "network.add_layer(1)\n",
    "network.train(X_train_scaled, y_train, minibatches=10, num_epochs = 200, learning_rate=0.001)\n",
    "y_pred = network.predict(X_train_scaled)\n",
    "MSE(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c4faea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398,\n",
       "       60.35599398, 60.35599398, 60.35599398, 60.35599398, 60.35599398])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a14dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5287755011500316"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = NeuralNetwork(X_train_scaled.shape[1], activation='leakyrelu')        \n",
    "network.add_layer(4)\n",
    "#network.add_layer(4)\n",
    "network.add_layer(1)\n",
    "network.train(X_train_scaled, y_train, minibatches=10, num_epochs = 200, learning_rate=0.0001)\n",
    "y_pred = network.predict(X_train_scaled)\n",
    "MSE(y_pred, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
