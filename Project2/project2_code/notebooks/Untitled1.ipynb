{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decb8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_path = os.getcwd()\n",
    "sys.path.append(current_path + '\\..')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Import self-made modules\n",
    "from project2_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a43d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.num_inputs = data.shape[1]\n",
    "        self.num_features = data.shape[0]\n",
    "        self.weights = {}  # Dictionary to store weights for each layer\n",
    "        self.activations = {} # Dictionary to store input for each layer\n",
    "        self.activations[0] = self.data \n",
    "        self.errors = {} \n",
    "        self.error_matrix = {}\n",
    "        self.activation_function = self.sigmoid\n",
    "        \n",
    "    def initialize_weights(self, size_layer, size_prev_layer, n):\n",
    "        # Use the He initializing for weights. Weights drawn from guassian dist with std sqrt(2/n)\n",
    "        return np.random.randn(size_layer, size_prev_layer) * np.sqrt(2/n)\n",
    "        \n",
    "    def add_layer(self, size_layer):\n",
    "        \"\"\"\n",
    "        Adds layer of size size_layer (bias excluded)\n",
    "        \"\"\"\n",
    "        # Check if this is first layer\n",
    "        if len(self.weights) == 0:\n",
    "            self.weights[0] = self.initialize_weights(size_layer, self.num_features, self.num_inputs)\n",
    "            self.error_matrix[0] = np.zeros((size_layer, self.num_features))\n",
    "        else:\n",
    "            counter = len(self.weights)\n",
    "            size_prev_layer = self.weights[counter - 1].shape[0]\n",
    "            self.weights[counter] = self.initialize_weights(size_layer, size_prev_layer + 1, self.num_inputs)\n",
    "            \n",
    "    def add_bias(self, x):\n",
    "        x = np.concatenate((np.ones((x.shape[1], 1)).T, x), axis=0)\n",
    "        return x\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        sigmoid = 1 / (1 + np.exp(-z))\n",
    "        return sigmoid\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "            \n",
    "    def forward_prop(self):\n",
    "        \"\"\"\n",
    "        Propagates input through all layers and return output of final layer\n",
    "        \"\"\"\n",
    "        current_layer = 0\n",
    "        num_layers = len(self.weights)\n",
    "        # Loop through all layers\n",
    "        while current_layer < num_layers:\n",
    "            #Get weights for current layer\n",
    "            weights = self.weights.get(current_layer)\n",
    "            #Get input data for current layer\n",
    "            inputs = self.activations.get(current_layer)\n",
    "            #Calculate weighted sum of input\n",
    "            z = weights @ inputs\n",
    "            #If layer is the not the last layer we add bias\n",
    "            if current_layer != num_layers:\n",
    "                z = self.add_bias(z)\n",
    "            #Calculate output using activation function\n",
    "            y = self.activation_function(z)\n",
    "            #Store output to use as input for next layer\n",
    "            current_layer += 1\n",
    "            self.activations[current_layer] = y\n",
    "\n",
    "    def back_prop(self, y):\n",
    "        current_layer = len(self.weights)\n",
    "        a = self.activations.get(current_layer)\n",
    "        print(a.shape)\n",
    "        error = (a - y)\n",
    "        print(error.shape)\n",
    "        self.errors[current_layer] = error\n",
    "        current_layer -= 1\n",
    "        while current_layer > 0:\n",
    "            a = self.activations.get(current_layer)\n",
    "            weights = self.weights.get(current_layer)\n",
    "            error_prev = self.errors.get(current_layer + 1)\n",
    "            error = np.dot(weights.T, error_prev)*self.sigmoid_derivative(a)\n",
    "            # Remove error corresponding to bias unit\n",
    "            error = error[1:, :]\n",
    "            self.errors[current_layer] = error\n",
    "            self.error_matrix[current_layer] = np.dot(error_prev, a.T)\n",
    "            current_layer -= 1\n",
    "        self.error_matrix[0] = np.dot(self.errors[1], self.activations[0].T)\n",
    "            \n",
    "    def update_weights(self, learning_rate, reg_coef):\n",
    "        current_layer = 0\n",
    "        while current_layer < len(self.weights):\n",
    "            m = self.num_inputs\n",
    "            size = self.weights[current_layer].shape\n",
    "            gradient = np.zeros(size)\n",
    "            gradient[:, 0] = 1/m * self.error_matrix[current_layer][:, 0]\n",
    "            gradient[:, 1:] = 1/m * (self.error_matrix[current_layer][:, 1:] + reg_coef*self.weights[current_layer][:, 1:])\n",
    "            self.weights[current_layer] -= learning_rate * gradient\n",
    "            current_layer += 1\n",
    "        \n",
    "    def train(self, num_epochs = 100, learning_rate = 1, reg_coef = 0):\n",
    "        for i in range(num_epochs):\n",
    "            # Get output by using feed forward\n",
    "            self.forward_prop()\n",
    "            # Propagate error using back propagation\n",
    "            self.back_prop(self.target)\n",
    "            # Update the weights\n",
    "            self.update_weights(learning_rate, reg_coef)\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epochs done: {i}/{num_epochs}')\n",
    "            \n",
    "    def predict(self, x):\n",
    "        self.activations[0] = self.add_bias(x)\n",
    "        self.forward_prop(x)\n",
    "        preds = self.activations[len(self.activations) - 1]\n",
    "        return np.argmax(preds, axis = 0)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        self.forward_prop(x)\n",
    "        preds = self.activations[len(self.activations) - 1]\n",
    "        return preds\n",
    "    \n",
    "    def accuracy(self, y_pred, y):\n",
    "        acc = 0\n",
    "        for i in range(len(y)):\n",
    "            if y_pred[i] == y[i]:\n",
    "                acc += 1\n",
    "        return acc/len(y)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = np.random.rand(n)\n",
    "\n",
    "# Create polynomial function of x, up to a degree of 5\n",
    "y = simple_polynomial(x, polynomial_degree = 2)\n",
    "X = create_design_matrix_1d(x, 2)\n",
    "#X.insert(0, 'bias', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4c4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1000)\n",
      "(11, 1000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (21,10) and (11,1000) not aligned: 10 (dim 1) != 11 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-733e8a6ac591>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-4a03ff2df2bb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_epochs, learning_rate, reg_coef)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# Propagate error using back propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[1;31m# Update the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_coef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4a03ff2df2bb>\u001b[0m in \u001b[0;36mback_prop\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0merror_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;31m# Remove error corresponding to bias unit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (21,10) and (11,1000) not aligned: 10 (dim 1) != 11 (dim 0)"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork(X.T, y.T)        \n",
    "network.add_layer(20)\n",
    "network.add_layer(10)\n",
    "network.train(1000, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf989be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
